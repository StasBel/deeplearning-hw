{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stasbel/.virtualenvs/deeplearning-hw-rnWr-itE/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import tqdm\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFC:\n",
    "    def __init__(self, name, inputs, outputs, lr):\n",
    "        self.name = name\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.lr = lr\n",
    "        \n",
    "        self._build()\n",
    "\n",
    "    def fit(self, sess, x, y):\n",
    "        sess.run(self.optimizer, feed_dict={self.x: x, self.y: y})\n",
    "    \n",
    "    def predict(self, sess, x):\n",
    "        return sess.run(self.y_pred, feed_dict={self.x: x})\n",
    "    \n",
    "    def _build(self):\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.inputs])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.outputs])\n",
    "        with tf.variable_scope(self.name):\n",
    "            net = slim.fully_connected(self.x, 25)\n",
    "            net = slim.fully_connected(net, 25)\n",
    "            self.y_pred = slim.fully_connected(net, self.outputs, activation_fn=None)\n",
    "            loss = tf.losses.mean_squared_error(self.y, self.y_pred)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.render = render\n",
    "    \n",
    "        # Contsts\n",
    "        self.discount_factor = 0.99\n",
    "        self.lr = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # Models\n",
    "#         tf.reset_default_graph()\n",
    "        self.model = IFC('model', state_size, action_size, self.lr)\n",
    "        self.target_model = IFC('target_model', state_size, action_size, self.lr)\n",
    "        \n",
    "        # Default TF pack\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter('./graphs', self.sess.graph)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'model')\n",
    "        tmodel_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'target_model')\n",
    "        for x, y in zip(model_vars, tmodel_vars):\n",
    "            self.sess.run(tf.assign(y, x.eval()))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(self.sess, state)[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(self.sess, state)[0]\n",
    "\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                max_q_value = np.max(self.target_model.predict(self.sess, next_state)[0])\n",
    "                target[action] = reward + self.discount_factor * max_q_value\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        self.model.fit(self.sess, update_input, update_target)\n",
    "\n",
    "    def load_model(self, base='./checkpoints', model=None):\n",
    "        try:\n",
    "            if model is not None:\n",
    "                last_chk_path = os.path.join(base, model)\n",
    "            else:\n",
    "                last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=base)\n",
    "            self.saver.restore(self.sess, save_path=last_chk_path)\n",
    "        except:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def save_model(self, base='./checkpoints', model='model'):\n",
    "        self.saver.save(self.sess, save_path=os.path.join(base, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = 2  # env.action_space.shape[0]\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model\n"
     ]
    }
   ],
   "source": [
    "agent = DeepQAgent(state_size, action_size)\n",
    "agent.load_model()\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "episode: 5   score: -200.0   memory length: 1177   epsilon: 0.9765777000000452\n",
      "episode: 10   score: -200.0   memory length: 2122   epsilon: 0.9577722000000815\n",
      "episode: 15   score: -200.0   memory length: 3075   epsilon: 0.938807500000118\n",
      "episode: 20   score: -200.0   memory length: 4075   epsilon: 0.9189075000001564\n",
      "episode: 25   score: -200.0   memory length: 5073   epsilon: 0.8990473000001947\n",
      "episode: 30   score: -200.0   memory length: 6073   epsilon: 0.8791473000002331\n",
      "episode: 35   score: -200.0   memory length: 7020   epsilon: 0.8603020000002695\n",
      "episode: 40   score: -200.0   memory length: 7974   epsilon: 0.8413174000003061\n",
      "episode: 45   score: -200.0   memory length: 8857   epsilon: 0.82374570000034\n",
      "episode: 50   score: -200.0   memory length: 9857   epsilon: 0.8038457000003784\n",
      "episode: 55   score: -200.0   memory length: 10000   epsilon: 0.7846223000004154\n",
      "episode: 60   score: -200.0   memory length: 10000   epsilon: 0.7647223000004538\n",
      "episode: 65   score: -200.0   memory length: 10000   epsilon: 0.7458571000004902\n",
      "episode: 70   score: -200.0   memory length: 10000   epsilon: 0.7270317000005265\n",
      "episode: 75   score: -200.0   memory length: 10000   epsilon: 0.7100371000005593\n",
      "episode: 80   score: -200.0   memory length: 10000   epsilon: 0.6901371000005977\n",
      "episode: 85   score: -200.0   memory length: 10000   epsilon: 0.6702371000006361\n",
      "episode: 90   score: -200.0   memory length: 10000   epsilon: 0.6512724000006727\n",
      "episode: 95   score: -200.0   memory length: 10000   epsilon: 0.6324669000007089\n",
      "episode: 100   score: -200.0   memory length: 10000   epsilon: 0.6135022000007455\n",
      "episode: 105   score: -181.0   memory length: 10000   epsilon: 0.5949355000007813\n",
      "episode: 110   score: -85.0   memory length: 10000   epsilon: 0.5791946000008117\n",
      "episode: 115   score: -163.0   memory length: 10000   epsilon: 0.5620209000008448\n",
      "episode: 120   score: -200.0   memory length: 10000   epsilon: 0.5433149000008809\n",
      "episode: 125   score: -195.0   memory length: 10000   epsilon: 0.526658600000913\n",
      "episode: 130   score: -194.0   memory length: 10000   epsilon: 0.5087486000009476\n",
      "episode: 135   score: -173.0   memory length: 10000   epsilon: 0.4907988000009565\n",
      "episode: 140   score: -200.0   memory length: 10000   epsilon: 0.47332660000094146\n",
      "episode: 145   score: -182.0   memory length: 10000   epsilon: 0.4556156000009262\n",
      "episode: 150   score: -199.0   memory length: 10000   epsilon: 0.4393175000009122\n",
      "episode: 155   score: -200.0   memory length: 10000   epsilon: 0.41941750000089506\n",
      "episode: 160   score: -172.0   memory length: 10000   epsilon: 0.4024030000008804\n",
      "episode: 165   score: -166.0   memory length: 10000   epsilon: 0.38556760000086593\n",
      "episode: 170   score: -200.0   memory length: 10000   epsilon: 0.36827450000085105\n",
      "episode: 175   score: -133.0   memory length: 10000   epsilon: 0.3555783000008401\n",
      "episode: 180   score: -162.0   memory length: 10000   epsilon: 0.3384643000008254\n",
      "episode: 185   score: -148.0   memory length: 10000   epsilon: 0.3213901000008107\n",
      "episode: 190   score: -200.0   memory length: 10000   epsilon: 0.30706210000079837\n",
      "episode: 195   score: -167.0   memory length: 10000   epsilon: 0.2935301000007867\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "#     print(state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 10:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            if e % 5 == 0:\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                      \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 20 == 0:\n",
    "        agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
